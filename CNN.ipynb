{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow.keras as keras\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras import models, optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aseem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aseem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aseem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "from csv import reader\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "\n",
    "#for amazon data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###preprocessing part, this is common to all three datasets\n",
    "def preprocess_text(text):\n",
    "    #convert the text to lowercase()\n",
    "    text = text.lower()\n",
    "    text = remove_tags(text)\n",
    "    \n",
    "    #remove all urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags = re.MULTILINE)\n",
    "    \n",
    "        \n",
    "    #remove users @ references and # from text\n",
    "#     text = re.sub(r'\\@\\w+|\\#', \"\", text)\n",
    "\n",
    "\n",
    "    #remove punctuations\n",
    "#     text = re.sub(r\"[^a-zA-Z. ]\",\"\",text)\n",
    "\n",
    "    #remove multiple fullstops\n",
    "#     text = re.sub(r'\\.+', \".\",text)\n",
    "\n",
    "    #remove all stopwords\n",
    "    text_tokens = word_tokenize(text)\n",
    "    filtered_words = [word for word in text_tokens if word not in stop_words]\n",
    "   \n",
    "    #stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    \n",
    "    #lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "    \n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "movie_reviews.isnull().values.any()\n",
    "\n",
    "movie_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews[\"review\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7efe5fefdc70>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='sentiment', data=movie_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Basically there a family where little boy Jake thinks there a zombie in his closet his parents are fighting all the time This movie is slower than soap opera and suddenly Jake decides to become Rambo and kill the zombie OK first of all when you re going to make film you must Decide if its thriller or drama As drama the movie is watchable Parents are divorcing arguing like in real life And then we have Jake with his closet which totally ruins all the film expected to see BOOGEYMAN similar movie and instead watched drama with some meaningless thriller spots out of just for the well playing parents descent dialogs As for the shots with Jake just ignore them '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = movie_reviews['sentiment']\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2951,  2084,  2583,  6223,  3461,    11,    20,  4213,  4364,\n",
       "           2,   394,     7,  1165, 10219,  3863,  2322,  2084,     5,\n",
       "         122,   687,   921,    18,  1861,    31,   672,   309,    51,\n",
       "           4,    22,  3274,     7,    44,  1638,  1165,   201, 13375,\n",
       "         237,    22, 14354,    20,  9195,    14,     1,   233,  1165,\n",
       "        9211,    52,    19,  7366,     4,   189,   141,     1,  1948,\n",
       "           2,     1,   141,     4,     1,  2533,  1165, 10219,     5,\n",
       "         482,    16,     6,  7458,   119,    32,   103,    73,   101,\n",
       "          46,    17,   130,  3160,     9,     6,   108,    55,   993,\n",
       "           4,  5875,     6,   200,  2117,     1,   222,     5,   670,\n",
       "        2976,     1,   201,     5,   482,    31,   540, 15165,   323,\n",
       "        4283,    35,     5,   161,    55,  2639,   215,   126,    17,\n",
       "          75,    70,   161,    40,     3,     8,   535,  6568,   757,\n",
       "         237,    27,    26,  2179,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-b63ce9b5a024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mglove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.840B.300d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglove_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "dim = 300\n",
    "glove_file = open('glove.840B.300d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    x = len(records) - dim\n",
    "    vector_dimensions = asarray(records[x:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, dim, weights=[embedding_matrix], trainable=False, input_length=maxlen))\n",
    "    model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#     model.compile(optimizer='adam',\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.00005),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 256, 300)          27703200  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 252, 128)          192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 27,896,629\n",
      "Trainable params: 193,429\n",
      "Non-trainable params: 27,703,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('final_models/CNN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1 = model.fit(X_train, y_train, batch_size=16, epochs=15, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Accuracy =\", history_1.history[\"val_accuracy\"][14:][0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.2817 - accuracy: 0.8958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.281716912984848, 0.895799994468689]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_models/CNN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_score(history):\n",
    "\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def compare_score(history_1, history_2):\n",
    "\n",
    "    plt.plot(history_1.history['accuracy'])\n",
    "    plt.plot(history_2.history['accuracy'])\n",
    "\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['model 1','model 2'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score(history_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aseem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aseem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aseem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from csv import reader\n",
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict ={}\n",
    "actualArray =[]\n",
    "predictedArray = []\n",
    "    \n",
    "with open('TestDataset - Sheet1.csv', 'r') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        row[1] = row[1].split(',')\n",
    "        actualArray.append(row[3])\n",
    "        my_dict[row[0]] = []\n",
    "        with open(row[4],'r') as file:\n",
    "            sentence = file.read()\n",
    "        sentences = sentence.split('.')\n",
    "        for sentence in sentences:\n",
    "            doc = nlp(sentence)\n",
    "            for ent in doc.ents:\n",
    "                if ent.text in row[1]:\n",
    "                    sentence = preprocess_text(sentence)\n",
    "                    my_dict[row[0]].append(sentence)\n",
    "\n",
    "# print(my_dict['Tony Stark'])\n",
    "final_dict = {}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony Stark\n",
      "Gabbar\n",
      "Sauron\n",
      "Khilji\n",
      "Bhallaladeva\n",
      "Amit Shellar\n",
      "Gandalf\n",
      "Raju\n",
      "Mogambo\n",
      "Kancha\n",
      "Kaal\n",
      "Loki\n",
      "Thanos\n",
      "Peter Parker\n",
      "Valentine\n",
      "Venom\n",
      "Otto Octavius\n",
      "Scar\n",
      "Simba\n",
      "Lady Tremaine\n",
      "Shere Khan\n",
      "Mowgli\n",
      "Sid Phillips\n",
      "Woody\n",
      "Evelyn\n",
      "Bob\n",
      "Dolores Umbridge\n",
      "Robert Callaghan\n",
      "Jafar\n",
      "Gaston\n",
      "Elsa\n",
      "Maleficent\n",
      "al Ghul\n",
      "Kaecilius\n",
      "Strange\n",
      "Batman\n",
      "Harry\n",
      "Amarendra Bahubali\n",
      "Bilbo Baggins\n",
      "Thor\n",
      "Saruman\n",
      "Frodo\n",
      "Farhan\n",
      "Louisa\n",
      "Biff Tannen\n",
      "Hans Gruber\n",
      "Chucky\n",
      "Jack Dawson\n",
      "William\n",
      "Mark Watney\n",
      "Rhett\n",
      "Jim\n",
      "Forrest\n",
      "Mia\n",
      "Simran\n",
      "Hazel\n",
      "Holmes\n",
      "John Watson\n",
      "Tim\n",
      "Mary\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "correct predictions are\n",
      "31\n",
      "accuracy is \n",
      "0.5166666666666667\n",
      "{'Tony Stark': 'good', 'Gabbar': 'good', 'Sauron': 'good', 'Khilji': 'good', 'Bhallaladeva': 'good', 'Amit Shellar': 'evil', 'Gandalf': 'good', 'Raju': 'good', 'Mogambo': 'good', 'Kancha': 'good', 'Kaal': 'good', 'Loki': 'good', 'Thanos': 'good', 'Peter Parker': 'good', 'Valentine': 'good', 'Venom': 'good', 'Otto Octavius': 'evil', 'Scar': 'good', 'Simba': 'good', 'Lady Tremaine': 'good', 'Shere Khan': 'good', 'Mowgli': 'good', 'Sid Phillips': 'evil', 'Woody': 'good', 'Evelyn': 'good', 'Bob': 'good', 'Dolores Umbridge': 'good', 'Robert Callaghan': 'good', 'Jafar': 'good', 'Gaston': 'good', 'Elsa': 'good', 'Maleficent': 'good', 'al Ghul': 'good', 'Kaecilius': 'good', 'Strange': 'good', 'Batman': 'good', 'Harry': 'good', 'Amarendra Bahubali': 'good', 'Bilbo Baggins': 'good', 'Thor': 'good', 'Saruman': 'good', 'Frodo': 'good', 'Farhan': 'good', 'Louisa': 'good', 'Biff Tannen': 'good', 'Hans Gruber': 'good', 'Chucky': 'good', 'Jack Dawson': 'good', 'William': 'good', 'Mark Watney': 'good', 'Rhett': 'good', 'Jim': 'good', 'Forrest': 'good', 'Mia': 'good', 'Simran': 'good', 'Hazel': 'good', 'Holmes': 'good', 'John Watson': 'good', 'Tim': 'good', 'Mary': 'good'}\n"
     ]
    }
   ],
   "source": [
    "for character in my_dict:\n",
    "    print(character)\n",
    "    positiveSentences = 0\n",
    "    negativeSentences = 0\n",
    "    coll = tokenizer.texts_to_sequences(my_dict[character])\n",
    "    coll = pad_sequences(coll, padding='post', maxlen=maxlen)\n",
    "    preds = model.predict(coll)\n",
    "#     print(preds)\n",
    "    for pred in preds:\n",
    "        if(pred>=0.5):\n",
    "            positiveSentences=positiveSentences+1\n",
    "        else:\n",
    "            negativeSentences=negativeSentences+1\n",
    "        ###write the code to assign sentiment to character here\n",
    "        ##increment positiveSentences and negativeSentences accordingly\n",
    "        \n",
    "    total = positiveSentences + negativeSentences\n",
    "    ##we have to vary this percentage and record stats\n",
    "    if negativeSentences >= 1*total:\n",
    "        final_dict[character] = \"evil\"\n",
    "    else:\n",
    "        final_dict[character] = \"good\"\n",
    "    predictedArray.append(final_dict[character])\n",
    "\n",
    "correct = 0\n",
    "length = len(actualArray)\n",
    "for i in range(length):\n",
    "    if actualArray[i]==predictedArray[i]:\n",
    "        correct = correct + 1\n",
    "        print(\"went here\")\n",
    "\n",
    "        \n",
    "accuracy = correct/length \n",
    "print(\"correct predictions are\")\n",
    "print(correct)\n",
    "print(\"accuracy is \")\n",
    "print(accuracy)\n",
    "\n",
    "print(final_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony Stark\n",
      "Gabbar\n",
      "Sauron\n",
      "Khilji\n",
      "Bhallaladeva\n",
      "Amit Shellar\n",
      "Gandalf\n",
      "Raju\n",
      "Mogambo\n",
      "Kancha\n",
      "Kaal\n",
      "Loki\n",
      "Thanos\n",
      "Peter Parker\n",
      "Valentine\n",
      "Venom\n",
      "Otto Octavius\n",
      "Scar\n",
      "Simba\n",
      "Lady Tremaine\n",
      "Shere Khan\n",
      "Mowgli\n",
      "Sid Phillips\n",
      "Woody\n",
      "Evelyn\n",
      "Bob\n",
      "Dolores Umbridge\n",
      "Robert Callaghan\n",
      "Jafar\n",
      "Gaston\n",
      "Elsa\n",
      "Maleficent\n",
      "al Ghul\n",
      "Kaecilius\n",
      "Strange\n",
      "Batman\n",
      "Harry\n",
      "Amarendra Bahubali\n",
      "Bilbo Baggins\n",
      "Thor\n",
      "Saruman\n",
      "Frodo\n",
      "Farhan\n",
      "Louisa\n",
      "Biff Tannen\n",
      "Hans Gruber\n",
      "Chucky\n",
      "Jack Dawson\n",
      "William\n",
      "Mark Watney\n",
      "Rhett\n",
      "Jim\n",
      "Forrest\n",
      "Mia\n",
      "Simran\n",
      "Hazel\n",
      "Holmes\n",
      "John Watson\n",
      "Tim\n",
      "Mary\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "correct predictions are\n",
      "31\n",
      "accuracy is \n",
      "0.5166666666666667\n",
      "{'Tony Stark': 'good', 'Gabbar': 'good', 'Sauron': 'good', 'Khilji': 'evil', 'Bhallaladeva': 'evil', 'Amit Shellar': 'evil', 'Gandalf': 'good', 'Raju': 'evil', 'Mogambo': 'good', 'Kancha': 'evil', 'Kaal': 'evil', 'Loki': 'evil', 'Thanos': 'evil', 'Peter Parker': 'good', 'Valentine': 'good', 'Venom': 'evil', 'Otto Octavius': 'evil', 'Scar': 'evil', 'Simba': 'good', 'Lady Tremaine': 'evil', 'Shere Khan': 'evil', 'Mowgli': 'good', 'Sid Phillips': 'evil', 'Woody': 'good', 'Evelyn': 'good', 'Bob': 'good', 'Dolores Umbridge': 'good', 'Robert Callaghan': 'good', 'Jafar': 'good', 'Gaston': 'evil', 'Elsa': 'good', 'Maleficent': 'evil', 'al Ghul': 'good', 'Kaecilius': 'good', 'Strange': 'evil', 'Batman': 'good', 'Harry': 'evil', 'Amarendra Bahubali': 'evil', 'Bilbo Baggins': 'good', 'Thor': 'evil', 'Saruman': 'good', 'Frodo': 'good', 'Farhan': 'evil', 'Louisa': 'good', 'Biff Tannen': 'good', 'Hans Gruber': 'good', 'Chucky': 'evil', 'Jack Dawson': 'good', 'William': 'evil', 'Mark Watney': 'evil', 'Rhett': 'evil', 'Jim': 'good', 'Forrest': 'good', 'Mia': 'good', 'Simran': 'good', 'Hazel': 'good', 'Holmes': 'evil', 'John Watson': 'evil', 'Tim': 'good', 'Mary': 'good'}\n"
     ]
    }
   ],
   "source": [
    "for character in my_dict:\n",
    "    print(character)\n",
    "    positiveSentences = 0\n",
    "    negativeSentences = 0\n",
    "    coll = tokenizer.texts_to_sequences(my_dict[character])\n",
    "    coll = pad_sequences(coll, padding='post', maxlen=maxlen)\n",
    "    preds = model.predict(coll)\n",
    "    for pred in preds:\n",
    "        if(pred>=0.5):\n",
    "            positiveSentences=positiveSentences+1\n",
    "        else:\n",
    "            negativeSentences=negativeSentences+1\n",
    "        ###write the code to assign sentiment to character here\n",
    "        ##increment positiveSentences and negativeSentences accordingly\n",
    "        \n",
    "    total = positiveSentences + negativeSentences\n",
    "    ##we have to vary this percentage and record stats\n",
    "    if negativeSentences >= 0.5*total:\n",
    "        final_dict[character] = \"evil\"\n",
    "    else:\n",
    "        final_dict[character] = \"good\"\n",
    "    predictedArray.append(final_dict[character])\n",
    "\n",
    "correct = 0\n",
    "length = len(actualArray)\n",
    "for i in range(length):\n",
    "    if actualArray[i]==predictedArray[i]:\n",
    "        correct = correct + 1\n",
    "        print(\"went here\")\n",
    "\n",
    "        \n",
    "accuracy = correct/length \n",
    "print(\"correct predictions are\")\n",
    "print(correct)\n",
    "print(\"accuracy is \")\n",
    "print(accuracy)\n",
    "\n",
    "print(final_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony Stark\n",
      "Gabbar\n",
      "Sauron\n",
      "Khilji\n",
      "Bhallaladeva\n",
      "Amit Shellar\n",
      "Gandalf\n",
      "Raju\n",
      "Mogambo\n",
      "Kancha\n",
      "Kaal\n",
      "Loki\n",
      "Thanos\n",
      "Peter Parker\n",
      "Valentine\n",
      "Venom\n",
      "Otto Octavius\n",
      "Scar\n",
      "Simba\n",
      "Lady Tremaine\n",
      "Shere Khan\n",
      "Mowgli\n",
      "Sid Phillips\n",
      "Woody\n",
      "Evelyn\n",
      "Bob\n",
      "Dolores Umbridge\n",
      "Robert Callaghan\n",
      "Jafar\n",
      "Gaston\n",
      "Elsa\n",
      "Maleficent\n",
      "al Ghul\n",
      "Kaecilius\n",
      "Strange\n",
      "Batman\n",
      "Harry\n",
      "Amarendra Bahubali\n",
      "Bilbo Baggins\n",
      "Thor\n",
      "Saruman\n",
      "Frodo\n",
      "Farhan\n",
      "Louisa\n",
      "Biff Tannen\n",
      "Hans Gruber\n",
      "Chucky\n",
      "Jack Dawson\n",
      "William\n",
      "Mark Watney\n",
      "Rhett\n",
      "Jim\n",
      "Forrest\n",
      "Mia\n",
      "Simran\n",
      "Hazel\n",
      "Holmes\n",
      "John Watson\n",
      "Tim\n",
      "Mary\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "correct predictions are\n",
      "31\n",
      "accuracy is \n",
      "0.5166666666666667\n",
      "{'Tony Stark': 'good', 'Gabbar': 'good', 'Sauron': 'good', 'Khilji': 'evil', 'Bhallaladeva': 'evil', 'Amit Shellar': 'evil', 'Gandalf': 'good', 'Raju': 'evil', 'Mogambo': 'good', 'Kancha': 'evil', 'Kaal': 'evil', 'Loki': 'evil', 'Thanos': 'evil', 'Peter Parker': 'good', 'Valentine': 'good', 'Venom': 'evil', 'Otto Octavius': 'evil', 'Scar': 'evil', 'Simba': 'good', 'Lady Tremaine': 'evil', 'Shere Khan': 'evil', 'Mowgli': 'good', 'Sid Phillips': 'evil', 'Woody': 'good', 'Evelyn': 'good', 'Bob': 'good', 'Dolores Umbridge': 'good', 'Robert Callaghan': 'good', 'Jafar': 'good', 'Gaston': 'evil', 'Elsa': 'good', 'Maleficent': 'evil', 'al Ghul': 'good', 'Kaecilius': 'good', 'Strange': 'evil', 'Batman': 'good', 'Harry': 'evil', 'Amarendra Bahubali': 'evil', 'Bilbo Baggins': 'good', 'Thor': 'evil', 'Saruman': 'good', 'Frodo': 'good', 'Farhan': 'evil', 'Louisa': 'good', 'Biff Tannen': 'good', 'Hans Gruber': 'good', 'Chucky': 'evil', 'Jack Dawson': 'good', 'William': 'evil', 'Mark Watney': 'evil', 'Rhett': 'evil', 'Jim': 'good', 'Forrest': 'good', 'Mia': 'good', 'Simran': 'good', 'Hazel': 'good', 'Holmes': 'evil', 'John Watson': 'evil', 'Tim': 'good', 'Mary': 'good'}\n"
     ]
    }
   ],
   "source": [
    "for character in my_dict:\n",
    "    print(character)\n",
    "    positiveSentences = 0\n",
    "    negativeSentences = 0\n",
    "    coll = tokenizer.texts_to_sequences(my_dict[character])\n",
    "    coll = pad_sequences(coll, padding='post', maxlen=maxlen)\n",
    "    preds = model.predict(coll)\n",
    "#     print(preds)\n",
    "    for pred in preds:\n",
    "        if(pred>=0.5):\n",
    "            positiveSentences=positiveSentences+1\n",
    "        else:\n",
    "            negativeSentences=negativeSentences+1\n",
    "        ###write the code to assign sentiment to character here\n",
    "        ##increment positiveSentences and negativeSentences accordingly\n",
    "        \n",
    "    total = positiveSentences + negativeSentences\n",
    "    ##we have to vary this percentage and record stats\n",
    "    if negativeSentences >= 0.5*total:\n",
    "        final_dict[character] = \"evil\"\n",
    "    else:\n",
    "        final_dict[character] = \"good\"\n",
    "    predictedArray.append(final_dict[character])\n",
    "\n",
    "correct = 0\n",
    "length = len(actualArray)\n",
    "for i in range(length):\n",
    "    if actualArray[i]==predictedArray[i]:\n",
    "        correct = correct + 1\n",
    "        print(\"went here\")\n",
    "\n",
    "        \n",
    "accuracy = correct/length \n",
    "print(\"correct predictions are\")\n",
    "print(correct)\n",
    "print(\"accuracy is \")\n",
    "print(accuracy)\n",
    "\n",
    "print(final_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony Stark\n",
      "Gabbar\n",
      "Sauron\n",
      "Khilji\n",
      "Bhallaladeva\n",
      "Amit Shellar\n",
      "Gandalf\n",
      "Raju\n",
      "Mogambo\n",
      "Kancha\n",
      "Kaal\n",
      "Loki\n",
      "Thanos\n",
      "Peter Parker\n",
      "Valentine\n",
      "Venom\n",
      "Otto Octavius\n",
      "Scar\n",
      "Simba\n",
      "Lady Tremaine\n",
      "Shere Khan\n",
      "Mowgli\n",
      "Sid Phillips\n",
      "Woody\n",
      "Evelyn\n",
      "Bob\n",
      "Dolores Umbridge\n",
      "Robert Callaghan\n",
      "Jafar\n",
      "Gaston\n",
      "Elsa\n",
      "Maleficent\n",
      "al Ghul\n",
      "Kaecilius\n",
      "Strange\n",
      "Batman\n",
      "Harry\n",
      "Amarendra Bahubali\n",
      "Bilbo Baggins\n",
      "Thor\n",
      "Saruman\n",
      "Frodo\n",
      "Farhan\n",
      "Louisa\n",
      "Biff Tannen\n",
      "Hans Gruber\n",
      "Chucky\n",
      "Jack Dawson\n",
      "William\n",
      "Mark Watney\n",
      "Rhett\n",
      "Jim\n",
      "Forrest\n",
      "Mia\n",
      "Simran\n",
      "Hazel\n",
      "Holmes\n",
      "John Watson\n",
      "Tim\n",
      "Mary\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "went here\n",
      "correct predictions are\n",
      "31\n",
      "accuracy is \n",
      "0.5166666666666667\n",
      "{'Tony Stark': 'good', 'Gabbar': 'good', 'Sauron': 'good', 'Khilji': 'evil', 'Bhallaladeva': 'evil', 'Amit Shellar': 'evil', 'Gandalf': 'good', 'Raju': 'good', 'Mogambo': 'good', 'Kancha': 'evil', 'Kaal': 'evil', 'Loki': 'good', 'Thanos': 'good', 'Peter Parker': 'good', 'Valentine': 'good', 'Venom': 'good', 'Otto Octavius': 'evil', 'Scar': 'evil', 'Simba': 'good', 'Lady Tremaine': 'evil', 'Shere Khan': 'evil', 'Mowgli': 'good', 'Sid Phillips': 'evil', 'Woody': 'good', 'Evelyn': 'good', 'Bob': 'good', 'Dolores Umbridge': 'good', 'Robert Callaghan': 'good', 'Jafar': 'good', 'Gaston': 'good', 'Elsa': 'good', 'Maleficent': 'evil', 'al Ghul': 'good', 'Kaecilius': 'good', 'Strange': 'evil', 'Batman': 'good', 'Harry': 'good', 'Amarendra Bahubali': 'evil', 'Bilbo Baggins': 'good', 'Thor': 'good', 'Saruman': 'good', 'Frodo': 'good', 'Farhan': 'evil', 'Louisa': 'good', 'Biff Tannen': 'good', 'Hans Gruber': 'good', 'Chucky': 'good', 'Jack Dawson': 'good', 'William': 'good', 'Mark Watney': 'evil', 'Rhett': 'good', 'Jim': 'good', 'Forrest': 'good', 'Mia': 'good', 'Simran': 'good', 'Hazel': 'good', 'Holmes': 'good', 'John Watson': 'evil', 'Tim': 'good', 'Mary': 'good'}\n"
     ]
    }
   ],
   "source": [
    "for character in my_dict:\n",
    "    print(character)\n",
    "    positiveSentences = 0\n",
    "    negativeSentences = 0\n",
    "    coll = tokenizer.texts_to_sequences(my_dict[character])\n",
    "    coll = pad_sequences(coll, padding='post', maxlen=maxlen)\n",
    "    preds = model.predict(coll)\n",
    "#     print(preds)\n",
    "    for pred in preds:\n",
    "        if(pred>=0.5):\n",
    "            positiveSentences=positiveSentences+1\n",
    "        else:\n",
    "            negativeSentences=negativeSentences+1\n",
    "        ###write the code to assign sentiment to character here\n",
    "        ##increment positiveSentences and negativeSentences accordingly\n",
    "        \n",
    "    total = positiveSentences + negativeSentences\n",
    "    ##we have to vary this percentage and record stats\n",
    "    if negativeSentences >= 0.6*total:\n",
    "        final_dict[character] = \"evil\"\n",
    "    else:\n",
    "        final_dict[character] = \"good\"\n",
    "    predictedArray.append(final_dict[character])\n",
    "\n",
    "correct = 0\n",
    "length = len(actualArray)\n",
    "for i in range(length):\n",
    "    if actualArray[i]==predictedArray[i]:\n",
    "        correct = correct + 1\n",
    "        print(\"went here\")\n",
    "\n",
    "        \n",
    "accuracy = correct/length \n",
    "print(\"correct predictions are\")\n",
    "print(correct)\n",
    "print(\"accuracy is \")\n",
    "print(accuracy)\n",
    "\n",
    "print(final_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
