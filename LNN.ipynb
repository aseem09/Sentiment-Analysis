{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "LNN.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2duFhE7Bn7mf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZYojwm0__y6",
        "outputId": "0beb883c-5e25-45d8-9ecb-fb6b86cd7c43"
      },
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from csv import reader\n",
        "import re\n",
        "import spacy\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdigFiLfAEvF"
      },
      "source": [
        "###preprocessing part, this is common to all three datasets\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = remove_tags(text)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # Single character removal\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyuOdrO7n7mn"
      },
      "source": [
        "UNIQUE_WORD_COUNT = 10000\n",
        "MAX_SEQUENCE_LENGTH = 64"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruhvso3R3w4g"
      },
      "source": [
        "# importing custom dataset\n",
        "word_index = imdb.get_word_index()\n",
        "# print(len(word_index.keys()))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnDAinJ9n7mr",
        "outputId": "07d89d1f-19e4-4caf-f7b8-641789ea1ae5"
      },
      "source": [
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=UNIQUE_WORD_COUNT)\n",
        "print(testing_data)\n",
        "print(testing_targets)\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[list([1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 7944, 451, 202, 14, 6, 717])\n",
            " list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 2, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 2, 19, 861, 1074, 5, 1987, 2, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 2, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 2, 5, 4182, 30, 3127, 2, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 2, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077])\n",
            " list([1, 111, 748, 4368, 1133, 2, 2, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 2, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 2, 16, 53, 928, 11, 2, 74, 4, 438, 21, 27, 2, 589, 8, 22, 107, 2, 2, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 2, 2425, 34, 2, 8738, 2, 5, 2, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 2, 1060, 63, 29, 93, 11, 5421, 11, 2, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 2, 10, 10, 4, 993, 2, 7, 4, 1766, 2634, 2164, 2, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 2, 16, 6, 465, 993, 2006, 2, 573, 17, 2, 42, 4, 2, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 2, 146, 655, 2212, 5, 258, 12, 184, 2, 546, 5, 849, 2, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 2, 5, 2, 11, 661, 8, 339, 2, 4, 2455, 2, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 2, 2, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 2, 2, 18, 6, 711, 4, 9909, 26, 2, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 2, 18, 27, 173, 9, 2399, 17, 6, 2, 428, 2, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 2, 54, 2, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 2, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 2, 3292, 98, 6, 2, 10, 10, 6639, 19, 14, 2, 267, 162, 711, 37, 5900, 752, 98, 4, 2, 2378, 90, 19, 6, 2, 7, 2, 1810, 2, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 2, 17, 2, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 2, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 2, 5437, 33, 1526, 6, 425, 3155, 2, 4535, 1636, 7, 4, 4669, 2, 469, 4, 4552, 54, 4, 150, 5664, 2, 280, 53, 2, 2, 18, 339, 29, 1978, 27, 7885, 5, 2, 68, 1830, 19, 6571, 2, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 2, 33, 4, 5673, 7, 15, 2, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 6676, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 2, 787, 7, 2460, 2, 2, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 2, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975])\n",
            " ...\n",
            " list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 2, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 2, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518])\n",
            " list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 2, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470])\n",
            " list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 2, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 2, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 2, 21, 4, 7298, 2, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 2, 2, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 2, 5698, 3406, 718, 2, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 2, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])]\n",
            "[0 1 1 ... 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWtCoHgXn7mx"
      },
      "source": [
        "def vectorize(sequences, dimension = UNIQUE_WORD_COUNT):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):      \n",
        "        results[i, sequence] = 1\n",
        "    return results"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94td7_Ckn7m1",
        "outputId": "b0ac5fa7-3649-4631-b612-443e4cacb6a1"
      },
      "source": [
        "print((data[1]))\n",
        "data = vectorize(data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s_VHRNKs3eL"
      },
      "source": [
        "targets = np.array(targets).astype(\"float32\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtL7eGgUs6iC",
        "outputId": "23deda64-e9fb-4b6c-b585-474c2a0df7b2"
      },
      "source": [
        "print(len(data))\n",
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "# valid_x = data[5000:10000]\n",
        "# valid_y = targets[5000:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tMsCjdUen7m4"
      },
      "source": [
        "# print(len(train_x))\n",
        "# print(len(valid_x))\n",
        "# print(len(test_x))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW28E0j-n7m8"
      },
      "source": [
        "def LNN():\n",
        "    model = models.Sequential()\n",
        "    \n",
        "    model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
        "    \n",
        "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(50, activation = \"relu\"))\n",
        "    model.add(layers.Dropout(0.5, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(50, activation = \"relu\"))\n",
        "    model.add(layers.Dropout(0.7, noise_shape=None, seed=None))\n",
        "    \n",
        "    model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "    model.summary()\n",
        "    \n",
        "    model.compile(\n",
        "     optimizer = optimizers.Adam(learning_rate=0.00005),\n",
        "     loss = \"binary_crossentropy\",\n",
        "     metrics = [\"accuracy\"]\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wqI2PXPn7nA",
        "outputId": "b1fe2c30-ebb8-46ce-9672-bdf2af3acf46"
      },
      "source": [
        "model_LNN = LNN()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                500050    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 505,201\n",
            "Trainable params: 505,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRlWKTwMn7nC",
        "outputId": "36b981aa-c458-4d53-e231-2a5fe6cfef25"
      },
      "source": [
        "results_LNN_train = model_LNN.fit(\n",
        " train_x, train_y,\n",
        " epochs= 8,\n",
        " batch_size = 32,\n",
        " validation_data = (test_x, test_y)\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "1250/1250 [==============================] - 7s 3ms/step - loss: 0.6694 - accuracy: 0.5708 - val_loss: 0.5585 - val_accuracy: 0.8186\n",
            "Epoch 2/8\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.4993 - accuracy: 0.7779 - val_loss: 0.3434 - val_accuracy: 0.8748\n",
            "Epoch 3/8\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.3711 - accuracy: 0.8615 - val_loss: 0.2834 - val_accuracy: 0.8885\n",
            "Epoch 4/8\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.3061 - accuracy: 0.8927 - val_loss: 0.2654 - val_accuracy: 0.8923\n",
            "Epoch 5/8\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.2668 - accuracy: 0.9081 - val_loss: 0.2620 - val_accuracy: 0.8941\n",
            "Epoch 6/8\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.2352 - accuracy: 0.9214 - val_loss: 0.2662 - val_accuracy: 0.8965\n",
            "Epoch 7/8\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.2084 - accuracy: 0.9305 - val_loss: 0.2696 - val_accuracy: 0.8961\n",
            "Epoch 8/8\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 0.1863 - accuracy: 0.9406 - val_loss: 0.2772 - val_accuracy: 0.8963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88SXXFJ1n7nF",
        "outputId": "3625fa0f-8067-4384-e133-e8a81965535c"
      },
      "source": [
        "results_LNN_test = model_LNN.evaluate(test_x, test_y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2772 - accuracy: 0.8963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BBetTqZn7nI",
        "outputId": "13654b1c-c457-4f8b-eafd-ff8bf4c2e3b5"
      },
      "source": [
        "print(results_LNN_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.2772058844566345, 0.8963000178337097]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOU0SsquHhif"
      },
      "source": [
        "my_dict ={}\n",
        "actualArray =[]\n",
        "predictedArray = []\n",
        "word_index = imdb.get_word_index()\n",
        "    \n",
        "with open('TestDataset - Sheet1.csv', 'r') as read_obj:\n",
        "    csv_reader = reader(read_obj)\n",
        "    next(csv_reader)\n",
        "    for row in csv_reader:\n",
        "        row[1] = row[1].split(',')\n",
        "        actualArray.append(row[3])\n",
        "        my_dict[row[0]] = []\n",
        "        with open(row[4],'r') as file:\n",
        "            sentence = file.read()\n",
        "        sentences = sentence.split('.')\n",
        "        for sentence in sentences:\n",
        "            doc = nlp(sentence)\n",
        "            for ent in doc.ents:\n",
        "                if ent.text in row[1]:\n",
        "                    sentence = preprocess_text(sentence)\n",
        "                    my_dict[row[0]].append(sentence)\n",
        "\n",
        "# print(my_dict['Tony Stark'])\n",
        "final_dict = {}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOFWrWh6II_6"
      },
      "source": [
        "def get_encoded_sentences(character):\n",
        "    encoded_sentences = []\n",
        "    for sentence in my_dict[character]:\n",
        "        words = sentence.split()\n",
        "        encoded_sentence = []\n",
        "        for word in words:\n",
        "            if word in word_index:\n",
        "                if word_index[word] < 10000:\n",
        "                    encoded_sentence.append(word_index[word])\n",
        "                else:\n",
        "                    encoded_sentence.append(0)    \n",
        "            else:\n",
        "                encoded_sentence.append(0)\n",
        "        encoded_sentences.append(encoded_sentence)\n",
        "    return encoded_sentences"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejkwi6Q65Fvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d39394-3ad5-496b-d568-3550400efb91"
      },
      "source": [
        "for character in my_dict:\n",
        "    # print(my_dict[character])\n",
        "    # print(character)\n",
        "    enc_sentences = get_encoded_sentences(character)\n",
        "    # print(enc_sentences)\n",
        "    positiveSentences = 0\n",
        "    negativeSentences = 0\n",
        "    prediction = model_LNN.predict(vectorize(np.asarray(enc_sentences)))\n",
        "    # print(prediction)\n",
        "    for pred in prediction:\n",
        "        ###write the code to assign sentiment to character here\n",
        "        ##increment positiveSentences and negativeSentences accordingly\n",
        "        # print(sentence)\n",
        "        if (pred>=0.5):\n",
        "            positiveSentences += 1\n",
        "        else:\n",
        "            negativeSentences += 1\n",
        "\n",
        "    total = positiveSentences + negativeSentences\n",
        "    ##we have to vary this percentage and record stats\n",
        "    if negativeSentences >= 0.25*total:\n",
        "        final_dict[character] = \"evil\"\n",
        "    else:\n",
        "        final_dict[character] = \"good\"\n",
        "    predictedArray.append(final_dict[character]) \n",
        "    \n",
        "\n",
        "correct = 0\n",
        "length = len(actualArray)\n",
        "for i in range(length):\n",
        "    if actualArray[i]==predictedArray[i]:\n",
        "        correct = correct + 1\n",
        "        # print(\"went here\")\n",
        "\n",
        "        \n",
        "accuracy = correct/length \n",
        "print(\"correct predictions are\")\n",
        "print(correct)\n",
        "print(\"accuracy is \")\n",
        "print(accuracy)\n",
        "\n",
        "print(final_dict)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "correct predictions are\n",
            "30\n",
            "accuracy is \n",
            "0.5\n",
            "{'Tony Stark': 'evil', 'Gabbar': 'evil', 'Sauron': 'evil', 'Khilji': 'evil', 'Bhallaladeva': 'evil', 'Amit Shellar': 'evil', 'Gandalf': 'evil', 'Raju': 'evil', 'Mogambo': 'evil', 'Kancha': 'evil', 'Kaal': 'evil', 'Loki': 'evil', 'Thanos': 'evil', 'Peter Parker': 'good', 'Valentine': 'evil', 'Venom': 'evil', 'Otto Octavius': 'evil', 'Scar': 'evil', 'Simba': 'evil', 'Lady Tremaine': 'evil', 'Shere Khan': 'evil', 'Mowgli': 'evil', 'Sid Phillips': 'good', 'Woody': 'evil', 'Evelyn': 'evil', 'Bob': 'evil', 'Dolores Umbridge': 'good', 'Robert Callaghan': 'evil', 'Jafar': 'evil', 'Gaston': 'evil', 'Elsa': 'evil', 'Maleficent': 'evil', 'al Ghul': 'evil', 'Kaecilius': 'evil', 'Strange': 'evil', 'Batman': 'evil', 'Harry': 'evil', 'Amarendra Bahubali': 'evil', 'Bilbo Baggins': 'evil', 'Thor': 'evil', 'Saruman': 'evil', 'Frodo': 'evil', 'Farhan': 'evil', 'Louisa': 'evil', 'Biff Tannen': 'evil', 'Hans Gruber': 'evil', 'Chucky': 'evil', 'Jack Dawson': 'evil', 'William': 'evil', 'Mark Watney': 'evil', 'Rhett': 'evil', 'Jim': 'evil', 'Forrest': 'evil', 'Mia': 'evil', 'Simran': 'evil', 'Hazel': 'evil', 'Holmes': 'evil', 'John Watson': 'evil', 'Tim': 'evil', 'Mary': 'evil'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0tf4Xyin7nL"
      },
      "source": [
        "# model_LNN.save('final_models/LNN/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-ZxWUCen7nN"
      },
      "source": [
        "preds_model = np.asarray(model_LNN(test_x))\n",
        "preds = []\n",
        "for i in preds_model:\n",
        "    if i>=0.5:\n",
        "        preds.append(1)\n",
        "    else:\n",
        "        preds.append(0)\n",
        "preds = np.asarray(preds)\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7UbPN74n7nV"
      },
      "source": [
        "test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jovAzhOpn7nZ"
      },
      "source": [
        "cm = confusion_matrix(test_y,preds)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YZzugSjn7nb"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.margins(x=1, y=1)\n",
        "#     plt.gcf().subplots_adjust(left=0.15)\n",
        "    plt.savefig('final_models/LNN/conf_matrix.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "WhFW9qOUn7n-"
      },
      "source": [
        "plot_confusion_matrix(cm, [\"Positive\", \"Negative\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWcTORe3n7oB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}